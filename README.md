# A Distributed Rough Set Theory based Algorithm for an Efficient Big Data Pre-processing under the Spark Framework

We have presented a novel efficient distributed algorithm based on Rough Set Theory for large-scale data pre-processing under the Spark framework. To reduce the computational effort of the rough set computations, our approach splits the given dataset into partitions with smaller numbers of features which are then processed in parallel.


## ACKNOWLEDGMENT
This work is part of a project that has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 702527.
